{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2a21224b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\swani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "import nltk.corpus\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial import distance\n",
    "from numpy.linalg import norm\n",
    "\n",
    "import time\n",
    "\n",
    "from multiprocessing.pool import ThreadPool as Pool\n",
    "from multiprocessing import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8bca2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"C:\\Users\\swani\\Python Projects\\Case Bucket\\Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dcaa4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Length: 114971\n",
      "   Unnamed: 0      Name  Article Number                          N-Grams\n",
      "0           0  20467935          538672            vhdl get csgmss error\n",
      "1           1  20467935          538672            get csgmss error data\n",
      "2           2  20467935          538672           csgmss error data type\n",
      "3           3  20467935          538672       error data type unresolved\n",
      "4           4  20467935          538672  data type unresolved std_ulogic\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_excel('N-Grams Data.xlsx')\n",
    "length = int(len(data))\n",
    "print('Data Length: %d' %(length))\n",
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c25356a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npool_size = 2  # your \"parallelness\"\\n# define worker function before a Pool is instantiated\\ndef worker(item):\\n    try:\\n        cd = cosine_similarity(item, input_vector)\\n        cosine_distance.append(cd)\\n    except:\\n        print(\\'error with item\\')\\n\\npool = Pool(pool_size)\\n\\nstart = time.time()\\nfor i in range(vectorizer.shape[0]):\\n    item = vectorizer[i].toarray()\\n    pool.apply_async(worker, (item,))\\n    \\npool.close()\\npool.join()\\nend = time.time()\\nprint(\\'\\')\\nprint(\\'Time Taken to Respond: %d seconds\\' %(end-start))\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "pool_size = 2  # your \"parallelness\"\n",
    "# define worker function before a Pool is instantiated\n",
    "def worker(item):\n",
    "    try:\n",
    "        cd = cosine_similarity(item, input_vector)\n",
    "        cosine_distance.append(cd)\n",
    "    except:\n",
    "        print('error with item')\n",
    "\n",
    "pool = Pool(pool_size)\n",
    "\n",
    "start = time.time()\n",
    "for i in range(vectorizer.shape[0]):\n",
    "    item = vectorizer[i].toarray()\n",
    "    pool.apply_async(worker, (item,))\n",
    "    \n",
    "pool.close()\n",
    "pool.join()\n",
    "end = time.time()\n",
    "print('')\n",
    "print('Time Taken to Respond: %d seconds' %(end-start))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "42c6b0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xcelium internal error\n",
      "\n",
      "Processed input: xcelium internal error\n",
      "\n",
      "[0.70710678]\n",
      "\n",
      "Indices of maximum value:  [62680, 87684]\n",
      "\n",
      "KB Articles: array([ 24840, 565812], dtype=int64)\n",
      "\n",
      "Output (Similar N-Grams): \n",
      "['error message internal error', 'xcelium xcelium issue error']\n",
      "\n",
      "Time Taken to Respond: 98 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "a = input()\n",
    "a = a.lower()\n",
    "a = a.replace('[^\\w\\s]','')\n",
    "stop_words = stopwords.words('english')\n",
    "a = [word for word in a.split() if word not in (stop_words)]\n",
    "a = ' '.join(a)\n",
    "lmtzr = WordNetLemmatizer()\n",
    "lemmatized = [lmtzr.lemmatize(word) for word in word_tokenize(a)]\n",
    "a = ' '.join(lemmatized)\n",
    "print('')\n",
    "print('Processed input: %s' %(a))\n",
    "\n",
    "ngrams = data['N-Grams']\n",
    "ngrams_list = ngrams.tolist()\n",
    "ngrams_list.append(a)\n",
    "\n",
    "vectorizer = CountVectorizer().fit_transform(ngrams_list)\n",
    "vectorizer = vectorizer.astype('uint8')\n",
    "input_vector = vectorizer[-1].toarray()\n",
    "vectorizer = vectorizer[:-1]\n",
    "\n",
    "def split(a, n):\n",
    "    k, m = divmod(len(a), n)\n",
    "    return (a[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n))\n",
    "splitList = list(split(range(vectorizer.shape[0]), 7))\n",
    "\n",
    "cosine_distance = []\n",
    "\n",
    "for i in splitList[0]:\n",
    "    cd = cosine_similarity(vectorizer[i], input_vector)\n",
    "    #cd = distance.cosine(vectorizer[i].toarray(), vectorizer[-1].toarray())\n",
    "    cosine_distance.append(cd)\n",
    "for i in splitList[1]:\n",
    "    cd = cosine_similarity(vectorizer[i], input_vector)\n",
    "    #cd = distance.cosine(vectorizer[i].toarray(), vectorizer[-1].toarray())\n",
    "    cosine_distance.append(cd)\n",
    "for i in splitList[2]:\n",
    "    cd = cosine_similarity(vectorizer[i], input_vector)\n",
    "    #cd = distance.cosine(vectorizer[i].toarray(), vectorizer[-1].toarray())\n",
    "    cosine_distance.append(cd)\n",
    "for i in splitList[3]:\n",
    "    cd = cosine_similarity(vectorizer[i], input_vector)\n",
    "    #cd = distance.cosine(vectorizer[i].toarray(), vectorizer[-1].toarray())\n",
    "    cosine_distance.append(cd)\n",
    "for i in splitList[4]:\n",
    "    cd = cosine_similarity(vectorizer[i], input_vector)\n",
    "    #cd = distance.cosine(vectorizer[i].toarray(), vectorizer[-1].toarray())\n",
    "    cosine_distance.append(cd)\n",
    "for i in splitList[5]:\n",
    "    cd = cosine_similarity(vectorizer[i], input_vector)\n",
    "    #cd = distance.cosine(vectorizer[i].toarray(), vectorizer[-1].toarray())\n",
    "    cosine_distance.append(cd)\n",
    "for i in splitList[6]:\n",
    "    cd = cosine_similarity(vectorizer[i], input_vector)\n",
    "    #cd = distance.cosine(vectorizer[i].toarray(), vectorizer[-1].toarray())\n",
    "    cosine_distance.append(cd)\n",
    "\n",
    "\n",
    "def get_maxvalue(inputlist):\n",
    " \n",
    "    #get the minimum value in the list\n",
    "    max_value = max(inputlist)\n",
    "    print('')\n",
    "    print(max_value[0])\n",
    " \n",
    "    #return the index of minimum value \n",
    "    res = [i for i,val in enumerate(inputlist) if val==max_value]\n",
    "    return res\n",
    "\n",
    "max_list = get_maxvalue(cosine_distance)\n",
    "print('')\n",
    "print(\"Indices of maximum value: \",max_list)\n",
    "\n",
    "lst = [data.iloc[i]['Article Number'] for i in max_list]\n",
    "KB_Articles = np.unique(lst)\n",
    "print('')\n",
    "print('KB Articles: %a' %(KB_Articles))\n",
    "print('')\n",
    "unique_indices = [lst.index(i) for i in KB_Articles]\n",
    "\n",
    "print('Output (Similar N-Grams): ')\n",
    "op_ngrams = []\n",
    "for i in unique_indices:\n",
    "    index = max_list[i]\n",
    "    op_ngrams.append(ngrams_list[index])\n",
    "print(op_ngrams)\n",
    "\n",
    "end = time.time()\n",
    "print('')\n",
    "print('Time Taken to Respond: %d seconds' %(end-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "11208e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7071067811865477"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(cosine_distance)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "923b5b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\na = input()\\na = a.lower()\\na = a.replace(\\'[^\\\\w\\\\s]\\',\\'\\')\\nstop_words = stopwords.words(\\'english\\')\\na = [word for word in a.split() if word not in (stop_words)]\\na = \\' \\'.join(a)\\nlmtzr = WordNetLemmatizer()\\nlemmatized = [lmtzr.lemmatize(word) for word in word_tokenize(a)]\\na = \\' \\'.join(lemmatized)\\nprint(\\'\\')\\nprint(\\'Processed input: %s\\' %(a))\\n\\nngrams = data[\\'N-Grams\\']\\nngrams_list = ngrams.tolist()\\nngrams_list.append(a)\\n\\nvectorizer = CountVectorizer().fit_transform(ngrams_list)\\nvectorizer = vectorizer.astype(\\'uint8\\')\\ninput_vector = vectorizer[-1].toarray()\\nvectorizer = vectorizer[:-1]\\n\\ncosine_distance = []\\nstart = time.time()\\nfor i in range(vectorizer.shape[0]):\\n    vectorizer[i].reshape(-1,1)\\n    cd = cosine_similarity(vectorizer[i], input_vector)\\n    #cd = distance.cosine(vectorizer[i].toarray(), vectorizer[-1].toarray())\\n    cosine_distance.append(cd)\\nend = time.time()\\nprint(\\'\\')\\nprint(\\'Time Taken to Respond: %d seconds\\' %(end-start))\\n\\ndef get_maxvalue(inputlist):\\n \\n    #get the minimum value in the list\\n    max_value = max(inputlist)\\n \\n    #return the index of minimum value \\n    res = [i for i,val in enumerate(inputlist) if val==max_value]\\n    return res\\n\\nmax_list = get_maxvalue(cosine_distance)\\nprint(\\'\\')\\nprint(\"Indices of maximum value: \",max_list)\\n\\nlst = [data.iloc[i][\\'Article Number\\'] for i in max_list]\\nKB_Articles = np.unique(lst)\\nprint(\\'\\')\\nprint(\\'KB Articles: %a\\' %(KB_Articles))\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "a = input()\n",
    "a = a.lower()\n",
    "a = a.replace('[^\\w\\s]','')\n",
    "stop_words = stopwords.words('english')\n",
    "a = [word for word in a.split() if word not in (stop_words)]\n",
    "a = ' '.join(a)\n",
    "lmtzr = WordNetLemmatizer()\n",
    "lemmatized = [lmtzr.lemmatize(word) for word in word_tokenize(a)]\n",
    "a = ' '.join(lemmatized)\n",
    "print('')\n",
    "print('Processed input: %s' %(a))\n",
    "\n",
    "ngrams = data['N-Grams']\n",
    "ngrams_list = ngrams.tolist()\n",
    "ngrams_list.append(a)\n",
    "\n",
    "vectorizer = CountVectorizer().fit_transform(ngrams_list)\n",
    "vectorizer = vectorizer.astype('uint8')\n",
    "input_vector = vectorizer[-1].toarray()\n",
    "vectorizer = vectorizer[:-1]\n",
    "\n",
    "cosine_distance = []\n",
    "start = time.time()\n",
    "for i in range(vectorizer.shape[0]):\n",
    "    vectorizer[i].reshape(-1,1)\n",
    "    cd = cosine_similarity(vectorizer[i], input_vector)\n",
    "    #cd = distance.cosine(vectorizer[i].toarray(), vectorizer[-1].toarray())\n",
    "    cosine_distance.append(cd)\n",
    "end = time.time()\n",
    "print('')\n",
    "print('Time Taken to Respond: %d seconds' %(end-start))\n",
    "\n",
    "def get_maxvalue(inputlist):\n",
    " \n",
    "    #get the minimum value in the list\n",
    "    max_value = max(inputlist)\n",
    " \n",
    "    #return the index of minimum value \n",
    "    res = [i for i,val in enumerate(inputlist) if val==max_value]\n",
    "    return res\n",
    "\n",
    "max_list = get_maxvalue(cosine_distance)\n",
    "print('')\n",
    "print(\"Indices of maximum value: \",max_list)\n",
    "\n",
    "lst = [data.iloc[i]['Article Number'] for i in max_list]\n",
    "KB_Articles = np.unique(lst)\n",
    "print('')\n",
    "print('KB Articles: %a' %(KB_Articles))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06df11dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cosine = np.dot(vectorizer.toarray(),input_vector.toarray().T)/(norm(vectorizer.toarray(), axis=1)*norm(input_vector.toarray().T))\n",
    "\n",
    "#cosine = np.dot(vectorizer,input_vector.T)/(norm(vectorizer, axis=1)*norm(input_vector.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "055871dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get covergroup report xcelium\n",
      "\n",
      "Processed input: get covergroup report xcelium\n"
     ]
    }
   ],
   "source": [
    "a = input()\n",
    "a = a.lower()\n",
    "a = a.replace('[^\\w\\s]','')\n",
    "stop_words = stopwords.words('english')\n",
    "a = [word for word in a.split() if word not in (stop_words)]\n",
    "a = ' '.join(a)\n",
    "lmtzr = WordNetLemmatizer()\n",
    "lemmatized = [lmtzr.lemmatize(word) for word in word_tokenize(a)]\n",
    "a = ' '.join(lemmatized)\n",
    "print('')\n",
    "print('Processed input: %s' %(a))\n",
    "\n",
    "ngrams = data['N-Grams']\n",
    "ngrams_list = ngrams.tolist()\n",
    "ngrams_list.append(a)\n",
    "\n",
    "vectorizer = TfidfVectorizer().fit_transform(ngrams_list)\n",
    "input_vector = vectorizer[-1]\n",
    "vectorizer = vectorizer[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4a10f8ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5243722634103872"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "cosine_similarities = linear_kernel(vectorizer[:], input_vector).flatten()\n",
    "max_value_cs = max(cosine_similarities)\n",
    "max_value_cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "049042e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [i for i,val in enumerate(cosine_similarities) if val==max_value_cs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "92a2b5e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18523]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1ecc09df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'covergroup coverage covergroup 0'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams_list[18523]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68a244da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkDuplicates(input_list): \n",
    "    for elem in input_list:\n",
    "        if input_list.count(elem) > 1:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a9b1237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkDuplicates(op_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d70d15d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'generated number heap object'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams_list[102327]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c0d313",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
