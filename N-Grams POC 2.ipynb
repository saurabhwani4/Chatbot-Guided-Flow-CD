{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a21224b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\swani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "import nltk.corpus\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial import distance\n",
    "from numpy.linalg import norm\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "import time\n",
    "import webbrowser \n",
    "\n",
    "from multiprocessing.pool import ThreadPool as Pool\n",
    "from multiprocessing import *\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8bca2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"C:\\Users\\swani\\Python Projects\\Case Bucket\\Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dcaa4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Length: 1918\n",
      "                                      Search Term  \\\n",
      "0                             probe depth working   \n",
      "1  using environment variables timing check tfile   \n",
      "2                            simulation profiling   \n",
      "3                            timescale overriding   \n",
      "4                          conditionally stopping   \n",
      "\n",
      "                    Search Term Cleaned  \n",
      "0                      probe depth work  \n",
      "1  use environ variabl time check tfile  \n",
      "2                          simul profil  \n",
      "3                      timescal overrid  \n",
      "4                           condit stop  \n"
     ]
    }
   ],
   "source": [
    "data = pd.read_excel('CD_UserQueries_clean.xlsx')\n",
    "length = int(len(data))\n",
    "print('Data Length: %d' %(length))\n",
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1bfeca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timescale overriding\n",
      "\n",
      "Processed input: timescale overriding\n",
      "[1.         0.73647798 0.51963309]\n",
      "\n",
      "Cosine Similarities:  [1.         0.73647798 0.51963309]\n",
      "[3]\n",
      "[200]\n",
      "[1347]\n",
      "\n",
      "Indices of 3 maximum value:  [3, 200, 1347]\n",
      "\n",
      "Output (Similar N-Grams): \n",
      "['timescale overriding', 'timescale ps', 'override module']\n",
      "\n",
      "Time Taken to Respond: 0 seconds\n"
     ]
    }
   ],
   "source": [
    "a = input()\n",
    "start = time.time()\n",
    "a = a.lower()\n",
    "a = a.replace('[^\\w\\s]','')\n",
    "stop_words = stopwords.words('english')\n",
    "a = [word for word in a.split() if word not in (stop_words)]\n",
    "a = ' '.join(a)\n",
    "lmtzr = WordNetLemmatizer()\n",
    "lemmatized = [lmtzr.lemmatize(word) for word in word_tokenize(a)]\n",
    "a = ' '.join(lemmatized)\n",
    "b = a\n",
    "stemmr = PorterStemmer()\n",
    "stemmed = [stemmr.stem(word) for word in word_tokenize(a)]\n",
    "a = ' '.join(stemmed)\n",
    "print('')\n",
    "print('Processed input: %s' %(b))\n",
    "\n",
    "searchTerm = data['Search Term Cleaned']\n",
    "searchTerm_list = searchTerm.tolist()\n",
    "searchTerm_display = data['Search Term']\n",
    "searchTerm_display_list = searchTerm_display.tolist()\n",
    "searchTerm_list.append(a)\n",
    "\n",
    "vectorizer = TfidfVectorizer().fit_transform(searchTerm_list)\n",
    "input_vector = vectorizer[-1]\n",
    "vectorizer = vectorizer[:-1]\n",
    "\n",
    "cosine_similarities = linear_kernel(vectorizer[:], input_vector).flatten()\n",
    "\n",
    "sort_cs = np.sort(cosine_similarities)[::-1]\n",
    "top3_values = sort_cs[:3]\n",
    "print(top3_values)\n",
    "if (any(i >= 0.5 for i in top3_values) is True):\n",
    "    pass\n",
    "else:\n",
    "    top3_values = [0,0,0]\n",
    "print('')\n",
    "print(\"Cosine Similarities: \",top3_values)\n",
    "\n",
    "indices_list = []\n",
    "\n",
    "if (all(i == 0 for i in top3_values) is True):\n",
    "    indices_list = [0,0,0]\n",
    "else:\n",
    "    for value in top3_values:\n",
    "        max_list = [i for i,val in enumerate(cosine_similarities) if val==value]\n",
    "        print(max_list)\n",
    "        if max_list[0] in indices_list:\n",
    "            if max_list[1] in indices_list:\n",
    "                indices_list.append(max_list[2])\n",
    "            else:\n",
    "                indices_list.append(max_list[1])\n",
    "        else: \n",
    "            indices_list.append(max_list[0])\n",
    "\n",
    "print('')\n",
    "print(\"Indices of 3 maximum value: \",indices_list)\n",
    "\n",
    "print('')\n",
    "print('Output (Similar N-Grams): ')\n",
    "op_suggestions = []\n",
    "if (indices_list == [0,0,0]):\n",
    "    print(\"No suggestions match the input query\")\n",
    "else:\n",
    "    for i in indices_list:\n",
    "        op_suggestions.append(searchTerm_display_list[i])\n",
    "    print(op_suggestions)\n",
    "\n",
    "end = time.time()\n",
    "print('')\n",
    "print('Time Taken to Respond: %d seconds' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6106144b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
